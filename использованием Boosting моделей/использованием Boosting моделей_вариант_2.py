# -*- coding: utf-8 -*-
"""ДЗ_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EiOW0pTX3WpRwlKL3r4L52VpW_FzrXhX
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from google.colab import files
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.stats import kurtosis
from scipy.stats import skew
from scipy.stats import shapiro
from scipy.stats import normaltest
import seaborn as sns
from sklearn.preprocessing import PowerTransformer
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from xgboost.sklearn import XGBClassifier
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
# %matplotlib inline

"""### решить задачу классификации на предложенном датасете с использованием Boosting моделей.

Датасет: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009

## Вспомогательные функции
"""

## Просмотр данных
def prosmotr(data):
  pd.set_option('display.max_columns', 100) #Размеры таблицы
  pd.set_option('display.max_rows', 100)
  pd.set_option('precision', 2) #Регулируем количество знаков после запятой:
  print('~~~~Содержание данных~~~~\n', data.head())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Размеры данных~~~\n', data.shape)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Названия колонок~~~\n', data.columns)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Информация о данных~~~\n')
  print(data.info())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Наличие пропусков в данных~~~\n', data.isna().sum())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Количество типов в данных~~~')
  print(data.dtypes.value_counts())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  kateg = list(data.select_dtypes(include=['object']).columns) # Делаем список категориальных данных
  print('~~~Категориальные данные~~~~')
  print(kateg)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  chislov_float = list(data.select_dtypes(include=['float64'])) #Делаем список числовых данных float
  print('~~~Числове данные float~~~~')
  print(chislov_float)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  chislov_int = list(data.select_dtypes(include=['int64'])) #Делаем список числовых данных int
  print('~~~Числове данные int~~~~')
  print(chislov_int)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Основные статистические характеристики данных по каждому числовому признаку (типы int64)~~~\n', data.describe(include=['int64']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Основные статистические характеристики данных по каждому числовому признаку (типы float64)~~~\n', data.describe(include=['float64']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')

## Анализ данных
def analyze(data):
  num = data.columns
  for i in num:
    print(i.title())
    print('~~~~~~~~~~~~~~~~~~~~~~~~~\n')
    print("mean : ", np.mean(data[i]))
    print("var  : ", np.var(data[i]))
    print("skew : ", skew(data[i]))
    print("kurt : ", kurtosis(data[i]))
    print("shapiro : ", shapiro(data[i]))
    print("normaltest : ", normaltest(data[i]))
    print('~~~~~~~~~~~~~~~~~~~~~~~~~')
    print('~~~~~~~~~~~~~~~~~~~~~~~~~\n')

# убираем лишее
def unpack_X_and_y(data, col):
  return data.drop([col],  axis=1), data[col]

# поиск лучших фич и удаляем лишнее
def best_features(X, y):
  #выбираем топ 10 фич
  bestfeatures = SelectKBest(score_func=chi2, k=10)
  fit = bestfeatures.fit(X,y)
  dfscores = pd.DataFrame(fit.scores_)
  dfcolumns = pd.DataFrame(X.columns)
  featureScores = pd.concat([dfcolumns,dfscores],axis=1)
  featureScores.columns = ['Specs','Score']  
  return list(featureScores.nlargest(10,'Score')['Specs'])

"""## По алгоритмам"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV


def fit_model(model, X, y, parameters):  
  cross_validation = StratifiedKFold(n_splits=5)

  grid_search = GridSearchCV(model,
                              scoring='accuracy',
                              param_grid=parameters,
                              cv=cross_validation,
                              verbose=1
                            )

  grid_search.fit(X, y)
  parameters=grid_search.best_params_
  print('Best score: {}'.format(grid_search.best_score_))
  print('Best parameters: {}'.format(parameters))

  return grid_search

file = files.upload()

#Loading dataset
df = pd.read_csv('winequality-red.csv')

df.head()

prosmotr(df)

plt.figure(figsize=(15,10))
corr = df.corr()
sns.heatmap(corr, cmap=sns.color_palette("RdBu_r", 1000), vmin=-1, center=0, annot=True)
plt.show()

df['quality'].hist()

analyze(df)

df['quality'].value_counts()

#Получаем таргет переменную и набор для обучения
x, y = unpack_X_and_y(df, 'quality')

x.shape

y.shape

x.head(3)

# стандартизируем
from sklearn import preprocessing
scaler = preprocessing.StandardScaler()
X = scaler.fit_transform(x)

X.shape

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)
sm = SMOTE()
X_train, y_train = sm.fit_sample(X_train, y_train)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

"""## Boost"""

model = XGBClassifier()
eval_set = [(X_test, y_test)]
model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=eval_set, verbose=False)

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

accuracy = metrics.accuracy_score(y_test, predictions)
print('Accuracy test: %.2f%%' % (accuracy * 100.0))
y_pred_train = model.predict(X_train)
predictions_train = [round(value) for value in y_pred_train]

accuracy = metrics.accuracy_score(y_train, predictions_train)
print('Accuracy train: %.2f%%' % (accuracy * 100.0))

from sklearn.ensemble import GradientBoostingRegressor
from sklearn import linear_model

params = {
    'n_estimators': 1,
    'max_depth': 1,
    'learning_rate': 1
}

gradient_boosting_regressor = GradientBoostingRegressor(**params)

gradient_boosting_regressor.fit(X_train, y_train)

y_pred = gradient_boosting_regressor.predict(X_test)
predictions = [round(value) for value in y_pred]

accuracy = metrics.accuracy_score(y_test, predictions)
print('Accuracy test: %.2f%%' % (accuracy * 100.0))
y_pred_train = gradient_boosting_regressor.predict(X_train)
predictions_train = [round(value) for value in y_pred_train]

accuracy = metrics.accuracy_score(y_train, predictions_train)
print('Accuracy train: %.2f%%' % (accuracy * 100.0))

"""## Еще один вариант"""

import xgboost as xgb
from sklearn import metrics


model = xgb.XGBClassifier()
param_dist = {"max_depth": [10,30,50],
              "min_child_weight" : [1,3,6],
              "n_estimators": [200],
              "learning_rate": [0.05, 0.1,0.16],}
model = GridSearchCV(model, param_grid=param_dist, cv = 5, 
                                   verbose=10, n_jobs=-1)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

accuracy = metrics.accuracy_score(y_test, predictions)
print('Accuracy test: %.2f%%' % (accuracy * 100.0))
y_pred_train = model.predict(X_train)
predictions_train = [round(value) for value in y_pred_train]

accuracy = metrics.accuracy_score(y_train, predictions_train)
print('Accuracy train: %.2f%%' % (accuracy * 100.0))

import lightgbm as lgb
from sklearn import metrics


lg = lgb.LGBMClassifier(silent=False)
param_dist = {"max_depth": [25,50, 75],
              "learning_rate" : [0.01,0.05,0.1],
              "num_leaves": [300,900,1200],
              "n_estimators": [200]
             }
model2 = GridSearchCV(lg, n_jobs=-1, param_grid=param_dist, cv = 5, verbose=5)
model2.fit(X_train,y_train)

y_pred = model2.predict(X_test)
predictions = [round(value) for value in y_pred]

accuracy = metrics.accuracy_score(y_test, predictions)
print('Accuracy test: %.2f%%' % (accuracy * 100.0))
y_pred_train = model2.predict(X_train)
predictions_train = [round(value) for value in y_pred_train]

accuracy = metrics.accuracy_score(y_train, predictions_train)
print('Accuracy train: %.2f%%' % (accuracy * 100.0))

!pip install catboost

import catboost as cb

params = {'depth': [4, 7, 10],
          'learning_rate' : [0.03, 0.1, 0.15],
         'l2_leaf_reg': [1,4,9],
         'iterations': [300]}
cb = cb.CatBoostClassifier()
cb_model = GridSearchCV(cb, params, cv = 5)
cb_model.fit(X_train, y_train)

y_pred = cb_model.predict(X_test)
predictions = [round(value) for value in y_pred]

accuracy = metrics.accuracy_score(y_test, predictions)
print('Accuracy test: %.2f%%' % (accuracy * 100.0))
y_pred_train = cb_model.predict(X_train)
predictions_train = [round(value) for value in y_pred_train]

accuracy = metrics.accuracy_score(y_train, predictions_train)
print('Accuracy train: %.2f%%' % (accuracy * 100.0))