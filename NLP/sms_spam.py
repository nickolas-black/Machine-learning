# -*- coding: utf-8 -*-
"""sms_spam.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBBBG58Prtcx0s8VSyldhwVa6BUjm84J

### Простой NLP пример

https://www.kaggle.com/uciml/sms-spam-collection-dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
from nltk.stem import WordNetLemmatizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB,  GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, recall_score, roc_curve, precision_score, auc, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline

def value_of_metrics(y_true, y_pred):
    print('Accuracy: ', accuracy_score(y_true, y_pred))
    print('Recall: ', recall_score(y_true, y_pred))
    print('Precision: ', precision_score(y_true, y_pred))  
    print('F1: ', f1_score(y_true, y_pred))
    print('Roc_AUC: ', roc_auc_score(y_true, y_pred))
    
    fpr, tpr, threshold = roc_curve(y_true, y_pred)
    roc_auc = auc(fpr, tpr)
    plt.title('My DataSet')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()

def text_preprocess(text):    
    reg = re.compile('[^а-яА-Яa-zA-Z0-9]') #
    text = text.lower()
    text = reg.sub(' ', text)
    lemmatizer = WordNetLemmatizer()
    text_ =''
    for word in text.split():
        #text_.append(lemmatizer.lemmatize(word))
        text_ = text_+ ' ' + lemmatizer.lemmatize(word)
    return text_

"""#### Загрузка данных из csv файла"""

df = pd.read_csv('sms/spam.csv', encoding="latin-1")
df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
df.head()

df['v2'].iloc[2]

"""#### Подготовка данных"""

df['label'] = df['v1'].map({'ham': 0, 'spam': 1})

df['v2'] = df['v2'].apply(text_preprocess)

df['v2'].iloc[2]

def f(x, ....):
    x[v1]
    x[v2]....

df['v2_new'] = df.apply(lambda x: f(x, ....... ) , axis = 1)

df.head()

df['label'].value_counts()

X = df['v2']
y = df['label']
#cv = CountVectorizer() # Bag of WORDS (BOW)
cv = TfidfVectorizer()
X = cv.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

clf = MultinomialNB()
clf.fit(X_train,y_train)
clf.score(X_test,y_test)
y_pred = clf.predict(X_test)

value_of_metrics(y_test, y_pred)

value_of_metrics(y_test, y_pred)

"""### Векторные представления документов с помощью SVD разложений"""

#обучаем tf_idf vectorizer ---------------------------------
tfidf = TfidfVectorizer()
sms_tf_idf = tfidf.fit_transform(df['v2'])
#обучаем tf_idf vectorizer ---------------------------------

X_train, X_test, y_train, y_test = train_test_split(sms_tf_idf, y, test_size=0.33, random_state=42)

#находим svd разложение---------------------------------------
svd = TruncatedSVD(80)
lsa = make_pipeline(svd, Normalizer(copy=False))
#находим svd разложение---------------------------------------

sms_svd_train = lsa.fit_transform(X_train)

sms_svd_train.shape

sms_svd_train[0]

#clf = SVC()
clf = GaussianNB()
clf.fit(sms_svd_train, y_train)

sms_svd_test = lsa.transform(X_test)

y_pred = clf.predict(sms_svd_test)

value_of_metrics(y_test, y_pred)

