# -*- coding: utf-8 -*-
"""Feature Selection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VmfrhrRBS7O8Fi4fwdIOK3LhPu_HAIPv

### Feature selection
"""

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

data = pd.read_csv("mobile-price-classification//train.csv")
data.head()

len(data)

"""- battery_power: Total energy a battery can store in one time measured in mAh
    
- blue: Has Bluetooth or not

- clock_speed: the speed at which microprocessor executes instructions

- dual_sim: Has dual sim support or not

- fc: Front Camera megapixels

- four_g: Has 4G or not

- int_memory: Internal Memory in Gigabytes

- m_dep: Mobile Depth in cm

- mobile_wt: Weight of mobile phone

- n_cores: Number of cores of the processor

- pc: Primary Camera megapixels

- px_height: Pixel Resolution Height

- px_width: Pixel Resolution Width

- ram: Random Access Memory in MegaBytes

- sc_h: Screen Height of mobile in cm

- sc_w: Screen Width of mobile in cm

- talk_time: the longest time that a single battery charge will last when you are

- three_g: Has 3G or not

- touch_screen: Has touch screen or not

- wifi: Has wifi or not

- price_range: This is the target variable with a value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).
"""

X = data.iloc[:,0:20]  #independent columns
y = data.iloc[:,-1]    #target column i.e price range

"""#### Univariate Selection"""

#apply SelectKBest class to extract top 10 best features
bestfeatures = SelectKBest(score_func=chi2, k=10)
fit = bestfeatures.fit(X,y)
dfscores = pd.DataFrame(fit.scores_)
dfcolumns = pd.DataFrame(X.columns)
#concat two dataframes for better visualization 
featureScores = pd.concat([dfcolumns,dfscores],axis=1)
featureScores.columns = ['Specs','Score']  #naming the dataframe columns
print(featureScores.nlargest(10,'Score'))  #print 10 best features

"""#### Feature Importance"""

from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt

model = ExtraTreesClassifier()
model.fit(X,y)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.show()

"""#### Correlation Matrix with Heatmap"""

import seaborn as sns
#get correlations of each features in dataset
corrmat = data.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap="RdYlGn")

"""#### Select From Model"""

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LassoCV

clf = LassoCV(cv=5)

sfm = SelectFromModel(clf) #threshold=0.4
sfm.fit(X, y)

n_features = sfm.transform(X).shape[1]

n_features

sfm.transform(X)

sfm.get_support()

pd.series(sfm.estimator_,feature_importances_,.ravel()).hist()