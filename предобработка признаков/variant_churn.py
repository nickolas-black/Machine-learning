# -*- coding: utf-8 -*-
"""ДЗ_№3_churn (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PfSMxpeJVFY1vaPx-ty4S8wwxyGCyoBU

### выполнить предобработку признаков данного датасета как числовых, так и категориальных. Далее выполнить нормировку признаков с помощью библиотеки scikit learn (StandartScalar) и обучить известные Вам модели бинарной классификации (kNN, Bayes, SVM).
"""

import numpy as np
import pandas as pd
from google.colab import files
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from scipy.stats import kurtosis
from scipy.stats import skew
from scipy.stats import shapiro
from scipy.stats import normaltest

"""## Создал функцию для удобства просмотра"""

## Просмотр данных
def prosmotr(data):
  pd.set_option('display.max_columns', 100) #Размеры таблицы
  pd.set_option('display.max_rows', 100)
  pd.set_option('precision', 2) #Регулируем количество знаков после запятой:
  print('~~~~Содержание данных~~~~\n', data.head())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Размеры данных~~~\n', data.shape)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Названия колонок~~~\n', data.columns)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Информация о данных~~~\n')
  print(data.info())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Наличие пропусков в данных~~~\n', data.isna().sum())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Количество типов в данных~~~')
  print(data.dtypes.value_counts())
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  kateg = list(data.select_dtypes(include=['object']).columns) # Делаем список категориальных данных
  print('~~~Категориальные данные~~~~')
  print(kateg)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  chislov_float = list(data.select_dtypes(include=['float64'])) #Делаем список числовых данных float
  print('~~~Числове данные float~~~~')
  print(chislov_float)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  chislov_int = list(data.select_dtypes(include=['int64'])) #Делаем список числовых данных int
  print('~~~Числове данные int~~~~')
  print(chislov_int)
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Основные статистические характеристики данных по каждому числовому признаку (типы int64)~~~\n', data.describe(include=['int64']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Основные статистические характеристики данных по каждому числовому признаку (типы float64)~~~\n', data.describe(include=['float64']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Cтатистика по нечисловым признакам object ~~~\n', data.describe(include=['object']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
  print('~~~Cтатистика по нечисловым признакам bool ~~~\n', data.describe(include=['bool']))
  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')

## Анализ данных
def analyze(data):
  num = data.columns
  for i in num:
    print(i.title())
    print('~~~~~~~~~~~~~~~~~~~~~~~~~\n')
    print("mean : ", np.mean(data[i]))
    print("var  : ", np.var(data[i]))
    print("skew : ", skew(data[i]))
    print("kurt : ", kurtosis(data[i]))
    print("shapiro : ", shapiro(data[i]))
    print("normaltest : ", normaltest(data[i]))
    print('~~~~~~~~~~~~~~~~~~~~~~~~~')
    print('~~~~~~~~~~~~~~~~~~~~~~~~~\n')

"""## Функции для работы с данными"""

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

# Для кодировки
def cod(data):
  cate=[key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['bool', 'object']]
  le = preprocessing.LabelEncoder()
  for i in cate:
    le.fit(data[i])
    data[i] = le.transform(data[i])
  return data

# Автоматическая нормировка данных
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
def standart(data):
  features_float = list(data.select_dtypes(include=['float']).columns) #выбираю колонки с float
  train_temp = data[features_float]
  scaler.fit(train_temp)
  train_temp_prep = scaler.transform(train_temp)
  t= pd.DataFrame(train_temp_prep)
  t.columns = features_float
  features_int = list(data.select_dtypes(include=['int']).columns) #выбираю колонки с int
  train_temp_int = data[features_int]
  scaler.fit(train_temp_int)
  train_temp_prep_int = scaler.transform(train_temp_int)
  i_t= pd.DataFrame(train_temp_prep_int)
  i_t.columns = features_int
  preob = pd.concat([t, i_t],axis=1)                           #сшиваю данные
  return preob

"""## Функции по алгоритмам"""

from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV


def fit_model(model, X, y, parameters):  
  cross_validation = StratifiedKFold(n_splits=5)

  grid_search = GridSearchCV(model,
                              scoring='accuracy',
                              param_grid=parameters,
                              cv=cross_validation,
                              verbose=1
                            )

  grid_search.fit(X, y)
  parameters=grid_search.best_params_
  print('Best score: {}'.format(grid_search.best_score_))
  print('Best parameters: {}'.format(parameters))

  return grid_search

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier

def KNN(train, targets):
  parameter_grid = {
                 'n_neighbors': [2, 5, 10, 15, 20, 25],
                 'metric': ['chebyshev', 'manhattan', 'euclidean', 'minkowski'],
                 'algorithm': ['ball_tree', 'kd_tree', 'brute']
                 }
  knn = KNeighborsClassifier()
  trained = fit_model(knn, train, targets, parameter_grid)
  return trained

"""## BAYES"""

from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB, ComplementNB


def Bern(train, targets):
  parameter_grid = {
                 'alpha': [0.001, 0.01, 0.1, 0.2, 0.5, 1.0],
                 'binarize': [0.0, 0.2, 0.5],
                 'fit_prior': ['True', 'False']
                 }
  bernoulli = BernoulliNB()
  trained_bernoulli = fit_model(bernoulli, train, targets, parameter_grid)
  return trained_bernoulli

def Gaus(train, targets):
  gaussian_nb = GaussianNB()
  parameter_grid = {
                 'var_smoothing': [1e-09, 1e-10, 1e-11, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]
                 }
  trained_gaussian_nb = fit_model(gaussian_nb, train, targets, parameter_grid)
  return trained_gaussian_nb

def Multi_nb(train, targets):
  multi_nb = MultinomialNB()
  parameter_grid = {
                 'alpha': [0.001, 0.01, 0.1, 0.2, 0.5, 1.0],
                 'fit_prior': ['True', 'False']
                 }
  trained_multi_nb = fit_model(multi_nb, train, targets, parameter_grid)
  return trained_multi_nb

def Complement_nb(train, targets):
  complement_nb = ComplementNB()
  parameter_grid = {
                 'alpha': [0.001, 0.01, 0.1, 0.2, 0.5, 1.0],
                 'fit_prior': ['True', 'False'],
                 'norm': ['True', 'False']
                 }
  trained_complement_nb = fit_model(complement_nb, train, targets, parameter_grid)
  return trained_complement_nb

"""## SVM"""

from sklearn import svm

def SVM(train, targets):
    Svm = svm.SVC(kernel='rbf')
    param_grid = {
        'C': [0.001, 0.01, 0.1, 1, 10],
        'gamma' : [0.001, 0.01, 0.1, 1]        
    }
    train_Svm = fit_model(Svm, train, targets,param_grid)
    return train_Svm

"""### Чтение из файла и первичный анализ"""

#file = files.upload()

#df = pd.read_csv('telecom_churn.csv')
df= pd.read_csv('/content/drive/My Drive/ugroz/dat.csv')

prosmotr(df)

df.head(3)

"""**Изменить тип колонки** можно с помощью метода `astype`. Применим этот метод к признаку `Churn` и переведём его в `int64`:"""

df['Churn'] = df['Churn'].astype('int64')

df.hist(figsize=(17,12))

"""## Кодируем данные"""

codir = cod(df)
codir.head(3)

"""# Выделим основную цель и уберем из тренировочного набора"""

y = df['Churn']
df = df.drop(['Churn'],axis=1)

y.head(3)

"""## Нормировка данных"""

dan = standart(df)
dan.head()

"""# Алгоритмы"""

KNN(dan, y)

Bern(dan, y)

Gaus(dan, y)

Multi_nb(df, y)

Complement_nb(df, y)

SVM(dan, y)

"""# Трансформация признаков, распределение которых далеко от нормального"""

from sklearn.preprocessing import PowerTransformer

box_cox_transform = PowerTransformer(method='box-cox', standardize=False) # only works with strictly positive values
yeo_johnson_transform = PowerTransformer(method='yeo-johnson', standardize=False) # works with positive and negative values
box = df
yeo_d = df
log_d = df

box['Account length'] = box_cox_transform.fit_transform(box['Account length'].values.reshape(box.shape[0],-1))
box['Total night charge'] = box_cox_transform.fit_transform(box['Total night charge'].values.reshape(box.shape[0],-1))

analyze(box)

KNN(box, y)

Bern(box, y)

Gaus(box, y)

SVM(box, y)

yeo_d['Account length'] = yeo_johnson_transform.fit_transform(yeo_d['Account length'].values.reshape(yeo_d.shape[0],-1))
yeo_d['Customer service calls'] = yeo_johnson_transform.fit_transform(yeo_d['Customer service calls'].values.reshape(yeo_d.shape[0],-1))

analyze(yeo_d)

KNN(yeo_d, y)

Bern(yeo_d, y)

Gaus(yeo_d, y)

SVM(yeo_d, y)

log_d['Total night charge'] = np.log(log_d['Total night charge'].values.reshape(log_d.shape[0],-1))
log_d['Total night calls'] = np.log(log_d['Total night calls'].values.reshape(log_d.shape[0],-1))

analyze(log_d)

KNN(log_d, y)

Bern(log_d, y)

Gaus(log_d, y)

SVM(log_d, y)