# -*- coding: utf-8 -*-
"""Feature_Preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IXJbmxCgG6FPxpUSsZnBNv4lb-Egq4PZ
"""

from IPython.display import Image
Image(url= "https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/5095eabce4b06cb305058603/5095eabce4b02d37bef4c24c/1352002236895/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg")

import pandas as pd
import numpy as np

train = pd.read_csv("titanic_train.csv")
test = pd.read_csv("titanic_test.csv")

train.head(10)

train.isnull().sum()

train['Age'] = train['Age'].fillna( train['Age'].mean() )

train[['Age','Fare']].hist()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd


from scipy.stats import kurtosis

'''
Kurtosis is the fourth central moment divided by the square of the variance. 
If Fisher’s definition is used, then 3.0 is subtracted from the result 
to give 0.0 for a normal distribution.

If bias is False then the kurtosis is calculated using k statistics 
to eliminate bias coming from biased moment estimators

Use kurtosistest to see if result is close enough to normal.
'''

from scipy.stats import skew
'''
For normally distributed data, the skewness should be about 0. 
A skewness value > 0 means that there is more weight in the left 
tail of the distribution. The function skewtest can be used to determine 
if the skewness value is close enough to 0, statistically speaking.
'''
from scipy.stats import shapiro

'''
For normally distributed data, the skewness should be about 0. 
A skewness value > 0 means that there is more weight in the left 
tail of the distribution. The function skewtest can be used to determine 
if the skewness value is close enough to 0, statistically speaking.

Returns:
W : float - The test statistic.

p-value : float - The p-value for the hypothesis test.

a : array_like, optional - If reta is True, then these are the internally computed “a” values 
that may be passed into this function on future calls.
'''



from scipy.stats import normaltest
'''
Test whether a sample differs from a normal distribution.

Returns:

statistic float or array
s^2 + k^2, where s is the z-score returned by skewtest and k is the z-score returned by kurtosistest.

pvaluefloat or array
A 2-sided chi squared probability for the hypothesis test.

If the p-val is very small, it means it is unlikely that the data came from a normal distribution.
'''

import matplotlib.pyplot as plt


x = np.random.normal(0, 2, 10000)   # create random values based on a normal distribution

def analyze(data):
    plt.style.use('ggplot')
    np.var(data)
    plt.hist(data, bins=60)
    print("mean : ", np.mean(data))
    print("var  : ", np.var(data))
    print("skew : ", skew(data))
    print("kurt : ", kurtosis(data))
    print("shapiro : ", shapiro(data))
    print("normaltest : ", normaltest(data))

x = np.random.normal( 0 , 1 , 100000 )
analyze(x)

x = np.random.normal(0, 1, 1000000)
analyze(x)

analyze(train['Age'])

analyze(train['Fare'])

"""### Приводим признаки к нормальному закону распределения"""

# Box-cox transformation
from sklearn.preprocessing import PowerTransformer

'''
Apply a power transform featurewise to make data more Gaussian-like.

Power transforms are a family of parametric, monotonic transformations 
that are applied to make data more Gaussian-like. This is useful for 
modeling issues related to heteroscedasticity (non-constant variance), 
or other situations where normality is desired.
'''
box_cox_transform = PowerTransformer(method='box-cox', standardize=False) # only works with strictly positive values
yeo_johnson_transform = PowerTransformer(method='yeo-johnson', standardize=False) # works with positive and negative values

train['Age_box_cox'] = box_cox_transform.fit_transform(train['Age'].values.reshape(train.shape[0],-1))
train['Age_yeo_johnson'] = yeo_johnson_transform.fit_transform(train['Age'].values.reshape(train.shape[0],-1))
train['Age_log'] = np.log(train['Age'].values.reshape(train.shape[0],-1))

analyze(train['Age_box_cox'])

analyze(train['Age_yeo_johnson'])

analyze(train['Age_log'])

#train['Fare_box_cox'] = box_cox_transform.fit_transform(train['Fare'].values.reshape(train.shape[0],-1))
train['Fare_yeo_johnson'] = yeo_johnson_transform.fit_transform(train['Fare'].values.reshape(train.shape[0],-1))

analyze(train['Fare_yeo_johnson'])

"""### Автоматические методы нормировки"""

features_float = list(train.select_dtypes(include=['float']).columns)
features_float

train_temp = train[features_float]
#test_temp = ....

from sklearn.preprocessing import StandardScaler # MinMaxScaler RobustScaler
scaler = StandardScaler()
scaler.fit(train_temp)

train_temp_prep = scaler.transform(train_temp)
#test_temp_prep = scaler.transform(test_temp)

train_temp_prep

