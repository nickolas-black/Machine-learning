# -*- coding: utf-8 -*-
"""Homework_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBTdU3Bt1wEth4WZOkbOGYOJeqMfSIMU

Выполнить задачу text summarization для своего набора статей на русском языке. 
В качестве векторных представлений предложений использовать либо предобученные 
модели https://rusvectores.org/ru/models/ или собственный embending, 
полученный из SVD разложения.
"""

import re
import numpy as np
import numpy as np
import pandas as pd
import nltk
import networkx as nx
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import cosine_similarity
nltk.download('punkt') # one time execution
nltk.download('stopwords')# one time execution

from google.colab import drive
drive.mount('/content/drive')

"""## Word embeddings"""

with open('/content/drive/My Drive/ML/Homework_9/model.txt', encoding='utf-8') as f:
  text = f.read()

text_list = re.sub('_VERB|_NOUN|_ADV|_ADJ|_INTJ|_PROPN|_NUM', '', text).split('\n')

word_embeddings = {}

for line in text_list[1:]:
  line_list = line.split(' ')
  word = line_list[0]
  vec = np.array(line_list[1:], dtype='float64')
  word_embeddings[word] = vec

"""## Статья"""

with open('/content/drive/My Drive/ML/Homework_9/Article.txt', 'r') as f:
  text = f.read()

text = re.sub('\n+', ' ', text)
sentences = sent_tokenize(text)

sentences = [re.sub("[^а-яА-Я]+", " ", i).lower() for i in sentences]

stop_words = stopwords.words('russian')

sno = nltk.stem.SnowballStemmer('russian')

def remove_stopwords(sen):
    sen_new = " ".join([sno.stem(i) for i in sen if sno.stem(i) not in stop_words])
    return sen_new

sentences_new = [remove_stopwords(r.split()) for r in sentences]

sentence_vectors = []

for i in sentences_new:
    
    if len(i) != 0:
        v = sum([word_embeddings.get(w, np.zeros((300,))) for w in i.split()])/(len(i.split())+0.001)
    else:
        v = np.zeros((300,))
    sentence_vectors.append(v)

sim_mat = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
    for j in range(len(sentences)):
        if i != j:
            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

for i in range(3):
    print(ranked_sentences[i][1])
    print('')

"""### SVD + TFIDF"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(sentences_new)

svd = TruncatedSVD(n_components=8, random_state=42)
sentence_vectors = normalize(svd.fit_transform(X))

sim_mat = np.zeros([len(sentences), len(sentences)])
for i in range(len(sentences)):
    for j in range(len(sentences)):
        if i != j:
            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 8), sentence_vectors[j].reshape(1, 8))[0,0]

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

for i in range(3):
    print(ranked_sentences[i][1])
    print('')

"""### TFIDF"""

sentence_vectors = X

sim_mat = np.zeros([len(sentences), len(sentences)])
for i in range(len(sentences)):
    for j in range(len(sentences)):
        if i != j:
            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1, 390), sentence_vectors[j].reshape(1, 390))[0,0]

nx_graph = nx.from_numpy_array(sim_mat)
scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

for i in range(3):
    print(ranked_sentences[i][1])
    print('')

"""### Текст"""

sentences

