# -*- coding: utf-8 -*-
"""09_text_summarization_pynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V74ZLnG3_fLxvfNVtLeFr3Z5QABmSwiy

## News text summarization
"""

# API e0207a9648fb4f85af10c206189158e9

"""## import"""

# общие
import requests
import numpy as np
import pandas as pd
import json
import csv

# для сохранения файлов
from google.colab import files

# для обработки текста
import nltk

from nltk.corpus import stopwords
from scipy.io import mmread
from nltk.tokenize import sent_tokenize
from gensim.summarization import keywords
from gensim.summarization.summarizer import summarize

from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
nltk.download('punkt') # one time execution
import re

"""## Get and save news"""

# url
url = 'https://newsapi.org/v2/top-headlines?country=us&apiKey=e0207a9648fb4f85af10c206189158e9'

# запрашиваем адрес
get = requests.get(url).json()

get

# метка
my_article = get['articles']

# создаем список для новостей
list1 = []

# функция парсинга новостей
def get_news(lst):
  for ar in my_article:
    news_dict=dict()
    
    news_dict['author'] = ar['author']
    news_dict['published At'] = ar['publishedAt']
    news_dict['title'] = ar['title']
    news_dict['content'] = ar['content']
    news_dict['url'] = ar['url']
    
    lst.append(news_dict)
    #print(news_dict)
  return

# получаем новости
get_news(list1)

# ключи для датафрейма csv
csv_keys = ['author', 'published At', 'title', 'content', 'url']

# создаем csv файл
with open('news.csv', 'w') as f:
    writer = csv.DictWriter(f, fieldnames=csv_keys)
    writer.writeheader()
    for item in list1:
        writer.writerow(item)

# сохраняем парсинг
files.download('news.csv')

data = pd.read_csv('news.csv')
data.head(3)

"""## text processing"""

# Разделение текстов на предложение

sentences = []
for i in data['content']:
  sentences.append(sent_tokenize(i))

# проверим предложения
sentences[0:10]

# flatten
sentences = [y for x in sentences for y in x ]

sentences[]

# Удаляем пунктуацию и лишние знаки, приводим к нижнему регистру

clean_sent = pd.Series(sentences).str.replace("[^a-zA-Z]", " ")
clean_sent = [s.lower() for s in clean_sent]

clean_sent[0]

# 
nltk.download('stopwords')

# stopwords
stop_words = stopwords.words('english')

# проверяем кол-во
len(stop_words)

# удаляем стоп слова из предложений
def remove_sw(sen):
  sen_new = " ".join([i for i in sen if i not in stop_words])
  return sen_new

clean_sent = [remove_sw(r.split()) for r in clean_sent]

# качаем Glove
!wget http://nlp.stanford.edu/data/glove.6B.zip

# Распаковываем Glove
!unzip glove.6B.zip

# создаем словарь с векторами слов
word_embeddings = {}

f = open('glove.6B.100d.txt', encoding='utf-8')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    word_embeddings[word] = coefs
f.close()

word_embeddings['cat']

sen_vectors = []


for i in clean_sent:
    
    if len(i) != 0:
        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)
    else:
        v = np.zeros((100,))
    sen_vectors.append(v)

len(sen_vectors)

sim_mat = np.zeros([len(sentences), len(sentences)])

for i in range(len(sentences)):
  for j in range(len(sentences)):
    if i != j:
      sim_mat[i][j] = cosine_similarity(sen_vectors[i].reshape(1,100), sen_vectors[j].reshape(1,100))[0,0]

nx_graph = nx.from_numpy_array(sim_mat)

scores = nx.pagerank(nx_graph)

ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)

len(ranked_sentences)

# Specify number of sentences to form the summary
sn = 10

# Generate summary
for i in range(sn):
    print(i,'=====\n',ranked_sentences[i][1])
    print('\n\nkewords:\n',keywords(ranked_sentences[i][1]))

