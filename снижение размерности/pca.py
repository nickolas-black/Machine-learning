# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2vCht1VyM-8SJ3oS6Nekmtc-ZMtLkEo
"""

import numpy as np
import matplotlib.pyplot as plt

"""### Дисперсия"""

mu, sigma = 0, 0.1 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)

count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
          linewidth=2, color='r')
plt.show()

mu, sigma = 0, 0.5 # mean and standard deviation
s = np.random.normal(mu, sigma, 1000)

count, bins, ignored = plt.hist(s, 30, density=True)
plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *
                np.exp( - (bins - mu)**2 / (2 * sigma**2) ),
         linewidth=2, color='r')
plt.show()

"""### Ковариационная матрица"""

mean = [0,0]
cov = [[1,0],[0,5]]

x,y = np.random.multivariate_normal(mean,cov,500).T
plt.plot(x,y,'x') 
plt.axis('equal') 
plt.show()

mean = [0,0]
cov = [[5,0],[0,1]]

x,y = np.random.multivariate_normal(mean,cov,500).T
plt.plot(x,y,'x') 
plt.axis('equal') 
plt.show()

"""### PCA"""

x = np.arange(1,11)
y = 2 * x + np.random.randn(10)*2
X = np.vstack((x,y))
print(X)

# Plot mock data
plt.figure(figsize=(10, 5))
plt.scatter(x, y)
plt.show()

Xcentered = (X[0] - x.mean(), X[1] - y.mean())
m = (x.mean(), y.mean())
print(Xcentered)
print("Mean vector: ", m)

# Plot mock data
plt.figure(figsize=(10, 5))
plt.scatter(Xcentered[0], Xcentered[1])
plt.show()

covmat = np.cov(Xcentered)
print(covmat, "\n")
print("Variance of X: ", np.cov(Xcentered)[0,0])
print("Variance of Y: ", np.cov(Xcentered)[1,1])
print("Covariance X and Y: ", np.cov(Xcentered)[0,1])

"""##### Собственные вектора и значения"""

_, vecs = np.linalg.eig(covmat)

vecs

origin = [0], [0] # origin point

plt.quiver(*origin, vecs[0], vecs[1], color=['r','r',], scale=21)
plt.show()

"""#### Находим проекции на оси"""

_, vecs = np.linalg.eig(covmat)
v = vecs[:,1]
Xnew = np.dot(v,Xcentered)
print(Xnew)

"""### Scikit-learn"""

from sklearn.decomposition import PCA
pca = PCA(n_components = 1)
XPCAreduced = pca.fit_transform(np.transpose(X))

print ('Our reduced X: \n', Xnew)
print ('Sklearn reduced X: \n', XPCAreduced)

print ('Mean vector: ', pca.mean_, m)
print ('Projection: ', pca.components_, v)
print ('Explained variance ratio: ', pca.explained_variance_ratio_)

"""### PCA и Iris"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns; sns.set(style='white')
# %matplotlib inline
from sklearn import decomposition
from sklearn import datasets
from mpl_toolkits.mplot3d import Axes3D

# Загрузим наши ириски
iris = datasets.load_iris()
X = iris.data
y = iris.target

# Заведём красивую трёхмерную картинку
fig = plt.figure(1, figsize=(6, 5))
plt.clf()
ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)

plt.cla()

for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:
    ax.text3D(X[y == label, 0].mean(),
              X[y == label, 1].mean() + 1.5,
              X[y == label, 2].mean(), name,
              horizontalalignment='center',
              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))
# Поменяем порядок цветов меток, чтобы они соответствовали правильному
y_clr = np.choose(y, [1, 2, 0]).astype(np.float)
ax.scatter(X[:, 3], X[:, 1], X[:, 2], c=y_clr, cmap=plt.cm.get_cmap("Spectral"))

ax.w_xaxis.set_ticklabels([])
ax.w_yaxis.set_ticklabels([])
ax.w_zaxis.set_ticklabels([])

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score

# Выделим из наших данных валидационную выборку
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, 
                                                    stratify=y, 
                                                    random_state=42)

# Для примера возьмём неглубокое дерево решений
clf = DecisionTreeClassifier(max_depth=2, random_state=42)
clf.fit(X_train, y_train)
preds = clf.predict_proba(X_test)
print('Accuracy: {:.5f}'.format(accuracy_score(y_test, 
                                                preds.argmax(axis=1))))

# Прогоним встроенный в sklearn PCA
pca = decomposition.PCA(n_components=2)
X_centered = X - X.mean(axis=0)
pca.fit(X_centered)
X_pca = pca.transform(X_centered)

# И нарисуем получившиеся точки в нашем новом пространстве
plt.plot(X_pca[y == 0, 0], X_pca[y == 0, 1], 'bo', label='Setosa')
plt.plot(X_pca[y == 1, 0], X_pca[y == 1, 1], 'go', label='Versicolour')
plt.plot(X_pca[y == 2, 0], X_pca[y == 2, 1], 'ro', label='Virginica')
plt.legend(loc=0);

# Повторим то же самое разбиение на валидацию и тренировочную выборку.
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, 
                                                    stratify=y, 
                                                    random_state=42)
clf = DecisionTreeClassifier(max_depth=2, random_state=42)
clf.fit(X_train, y_train)
preds = clf.predict_proba(X_test)
print('Accuracy: {:.5f}'.format(accuracy_score(y_test, 
                                                preds.argmax(axis=1))))

for i, component in enumerate(pca.components_):
    print("{} component: {}% of initial variance".format(i + 1, 
          round(100 * pca.explained_variance_ratio_[i], 2)))
    print(" + ".join("%.3f x %s" % (value, name)
                     for value, name in zip(component,
                                            iris.feature_names)))

"""### PCA and digits"""

digits = datasets.load_digits()
X = digits.data
y = digits.target

# f, axes = plt.subplots(5, 2, sharey=True, figsize=(16,6))
plt.figure(figsize=(16, 6))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X[i,:].reshape([8,8]));

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# pca = decomposition.PCA(n_components=2)
# X_reduced = pca.fit_transform(X)
# 
# print('Projecting %d-dimensional data to 2D' % X.shape[1])
# 
# plt.figure(figsize=(12,10))
# plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, 
#             edgecolor='none', alpha=0.7, s=40,
#             cmap=plt.cm.get_cmap('nipy_spectral', 10))
# plt.colorbar()
# plt.title('MNIST. PCA projection')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# from sklearn.manifold import TSNE
# tsne = TSNE(random_state=17)
# 
# X_tsne = tsne.fit_transform(X)
# 
# plt.figure(figsize=(12,10))
# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, 
#             edgecolor='none', alpha=0.7, s=40,
#             cmap=plt.cm.get_cmap('nipy_spectral', 10))
# plt.colorbar()
# plt.title('MNIST. t-SNE projection')

pca = decomposition.PCA().fit(X)

plt.figure(figsize=(10,7))
plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2)
plt.xlabel('Number of components')
plt.ylabel('Total explained variance')
plt.xlim(0, 63)
plt.yticks(np.arange(0, 1.1, 0.1))
plt.axvline(27, c='b')
plt.axhline(0.95, c='r')
plt.show();

np.cumsum(pca.explained_variance_ratio_)

